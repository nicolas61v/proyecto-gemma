# ğŸ¤– Chat Local con Gemma 3 270M Instruct

**Proyecto de EspecializaciÃ³n en Small Language Models (SLM)**
Universidad EAFIT, 2025

Autores: Felipe Castro Jaimes, NicolÃ¡s VÃ¡zquez, JosÃ© JimÃ©nez

---

## ğŸ“‹ DescripciÃ³n del Proyecto

Este es un **chatbot local e inteligente** basado en Gemma 3 270M (modelo instruction-tuned de Google). La aplicaciÃ³n ofrece:

âœ… **Chat interactivo** con interfaz web (Gradio)
âœ… **RAG (Retrieval-Augmented Generation)** - indexaciÃ³n local de documentos
âœ… **Soporte para fine-tuning** con QLora
âœ… **MÃºltiples formatos** - TXT, MD, PDF
âœ… **Totalmente local** - Sin dependencias de APIs externas

---

## ğŸš€ Inicio RÃ¡pido (Windows)

### 1ï¸âƒ£ Requisitos Previos

- **Python 3.8+** ([descargar](https://www.python.org/downloads/))
- **Token de Hugging Face** (gratuito, [registrarse aquÃ­](https://huggingface.co/))
- **~10 GB de espacio en disco** (para descargar el modelo)

### 2ï¸âƒ£ InstalaciÃ³n

#### OpciÃ³n A: Script automÃ¡tico (recomendado en Windows)

```bash
instalar_windows_gemma3.bat
```

Este script:
- âœ… Crea entorno virtual
- âœ… Instala todas las dependencias
- âœ… Configura la aplicaciÃ³n

#### OpciÃ³n B: InstalaciÃ³n manual

```bash
# Crear entorno virtual
python -m venv venv

# Activar entorno (Windows)
venv\Scripts\activate

# Activar entorno (Linux/Mac)
source venv/bin/activate

# Instalar dependencias
pip install -r requirements_gemma3.txt
```

### 3ï¸âƒ£ Configurar Token de Hugging Face

Necesitas acceso al modelo de Google en Hugging Face:

```bash
huggingface-cli login
# Pega tu token cuando se te pida
```

**Â¿DÃ³nde conseguir el token?**
1. Ve a https://huggingface.co/settings/tokens
2. Crea un nuevo token (read)
3. Pegalo en la terminal
4. u usa esta REDACTED

### 4ï¸âƒ£ Ejecutar la AplicaciÃ³n

#### Windows:
```bash
ejecutar_gemma3.bat
```

#### Linux/Mac:
```bash
python gemma3_270m_chat.py
```

**Resultado esperado:**
```
==============================================================
ğŸš€ CHAT LOCAL CON GEMMA 3 270M INSTRUCT
   Proyecto SLM - Universidad EAFIT 2025
==============================================================
ğŸ“± Dispositivo: CUDA (o CPU)
ğŸ”§ Modelo: google/gemma-3-270m-it
âœ… Usando versiÃ³n INSTRUCTION-TUNED

â³ Cargando Gemma 3 270M Instruct...
   (Primera vez descargarÃ¡ ~241MB)

âœ… Â¡Gemma 3 270M Instruct cargado exitosamente!

ğŸŒ Abriendo interfaz web en http://127.0.0.1:7860
```

Luego abre en tu navegador: **http://127.0.0.1:7860**

---

## ğŸ’¡ CaracterÃ­sticas Principales

### 1. Chat Interactivo
- Respuestas coherentes y contextuales
- Historial de conversaciÃ³n (Ãºltimas 3 interacciones)
- Controles de temperatura y lÃ­mite de tokens

### 2. RAG (Memoria con Documentos)
Permite que el modelo responda preguntas basadas en tus documentos:

**Pasos:**
1. Coloca archivos en la carpeta `knowledge/` (TXT, MD, PDF)
2. O sube archivos directamente en la interfaz
3. El sistema indexa automÃ¡ticamente con FAISS
4. Las respuestas incluyen contexto de tus documentos

**Archivos soportados:**
- `.txt` - Archivos de texto
- `.md` - Markdown
- `.pdf` - Documentos PDF

**Estructura de directorios:**
```
proyecto-gemma/
â”œâ”€â”€ knowledge/              # Coloca tus documentos aquÃ­
â”‚   â”œâ”€â”€ documento1.txt
â”‚   â”œâ”€â”€ documento2.md
â”‚   â””â”€â”€ documento3.pdf
â”œâ”€â”€ knowledge_index.faiss   # Ãndice (se crea automÃ¡ticamente)
â”œâ”€â”€ knowledge_embeddings.npy
â””â”€â”€ knowledge_metadata.json
```

### 3. Fine-tuning con QLora
Entrena el modelo con tus propios datos:

**Preparar datos:**
Crea un archivo `train.jsonl`:
```json
{"prompt": "Â¿QuÃ© es IA?", "response": "La IA es..."}
{"prompt": "Â¿CÃ³mo funciona?", "response": "Funciona mediante..."}
```

**Entrenar** (recomendado en Linux/WSL2):
```bash
python qlora_finetune.py --train_file train.jsonl --output_dir lora_adapter --num_epochs 3
```

El modelo cargarÃ¡ automÃ¡ticamente el adaptador LoRA si existe.

---

## âš™ï¸ ConfiguraciÃ³n Avanzada

### Ajustar ParÃ¡metros del Modelo

Edita `gemma3_270m_chat.py`:

```python
# LÃ­nea 40 - Cambiar modelo
MODEL_NAME = "google/gemma-3-270m-it"  # Usa este version!

# LÃ­nea 47 - Modelo de embeddings para RAG
EMBED_MODEL_NAME = "all-MiniLM-L6-v2"

# LÃ­nea 41 - Dispositivo
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
```

### Optimizar para CPU
Si no tienes GPU, aÃ±ade en `gemma3_270m_chat.py` (lÃ­nea 67-75):

```python
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float32,  # Cambia float16 â†’ float32 para CPU
    device_map=None,
    low_cpu_mem_usage=True
)
```

---

## ğŸ› SoluciÃ³n de Problemas

### âŒ "ModuleNotFoundError"
**SoluciÃ³n:** AsegÃºrate de activar el entorno virtual
```bash
# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate
```

### âŒ "No authorization token provided"
**SoluciÃ³n:** Configura tu token de Hugging Face
```bash
huggingface-cli login
```

### âŒ CUDA out of memory
**SoluciÃ³n:** Reduce los tokens mÃ¡ximos en la interfaz o usa CPU

### âŒ Respuestas extraÃ±as
**Importante:** AsegÃºrate de usar `google/gemma-3-270m-it` (instruction-tuned)
- âœ… `gemma-3-270m-it` - Sigue instrucciones correctamente
- âŒ `gemma-3-270m` - Solo continÃºa texto, no sigue instrucciones

---

## ğŸ“ Estructura del Proyecto

```
proyecto-gemma/
â”œâ”€â”€ gemma3_270m_chat.py           # ğŸ¯ AplicaciÃ³n principal
â”œâ”€â”€ qlora_finetune.py             # Entrenamiento con QLora
â”œâ”€â”€ index_knowledge.py            # ConstrucciÃ³n de Ã­ndice FAISS
â”œâ”€â”€ requirements_gemma3.txt       # Dependencias Python
â”œâ”€â”€ instalar_windows_gemma3.bat   # Instalador Windows
â”œâ”€â”€ ejecutar_gemma3.bat           # Ejecutor Windows
â”œâ”€â”€ .gitignore                    # Git ignore
â”œâ”€â”€ README.md                     # Este archivo
â”œâ”€â”€ README_QLoRA.md               # GuÃ­a detallada de QLora
â””â”€â”€ knowledge/                    # Documentos (tÃº creas esta carpeta)
```

---

## ğŸ“¦ Dependencias

**NÃºcleo:**
- `torch` - Framework de deep learning
- `transformers` - Modelos de Hugging Face
- `gradio` - Interfaz web

**Opcional pero recomendado:**
- `sentence-transformers` - Embeddings para RAG
- `faiss-cpu` - BÃºsqueda de similaridad
- `peft` - QLora fine-tuning
- `bitsandbytes` - Optimizaciones de entrenamiento

**ConversiÃ³n de archivos:**
- `pypdf` - Lectura de PDFs

---

## ğŸ“ Primeros Pasos

### Prueba 1: Chat Simple
```
Usuario: Hola, Â¿cÃ³mo estÃ¡s?
Bot: Hola! Estoy bien, gracias por preguntar...
```

### Prueba 2: Preguntas de Conocimiento
```
Usuario: Â¿QuÃ© es un transformer en IA?
Bot: Un transformer es una arquitectura de red neuronal...
```

### Prueba 3: Usar RAG
1. Sube un PDF o TXT con informaciÃ³n
2. Pregunta algo relacionado
3. El modelo responderÃ¡ basÃ¡ndose en tu documento

---

## ğŸ” InformaciÃ³n del Sistema

| Componente | Detalles |
|-----------|----------|
| **Modelo** | Gemma 3 270M Instruction-Tuned |
| **ParÃ¡metros** | 270 millones |
| **Entrenamiento** | 6 trillones de tokens |
| **Tipo** | Instruction-Tuned (sigue instrucciones) |
| **Interfaz** | Gradio (web) |
| **RAG** | FAISS + Sentence Transformers |
| **Entrenamiento** | QLora (4-bit) |

---

## ğŸ’» Requisitos del Sistema

### MÃ­nimos
- CPU moderna (Intel/AMD)
- 8 GB RAM
- 10 GB disco
- Python 3.8+

### Recomendados
- **GPU NVIDIA** (CUDA 11.8+)
- 16 GB RAM
- 20 GB SSD
- Python 3.10+

### Ã“ptimos
- **GPU NVIDIA RTX 3060+**
- 32 GB RAM
- 100 GB SSD
- Windows 11 o Ubuntu 22.04+

---

## ğŸ¤ Contribuciones y Mejoras

Posibles mejoras futuras:
- [ ] Interfaz mejorada (FastAPI)
- [ ] Soporte para mÃ¡s modelos SLM
- [ ] Indexador FAISS offline mÃ¡s eficiente
- [ ] Dashboard de estadÃ­sticas
- [ ] API REST

---

## ğŸ“ Licencia

Este proyecto es parte del programa de especializaciÃ³n en SLM de EAFIT 2025.

---

## ğŸ“ Soporte

Para problemas o preguntas:
1. Revisa la secciÃ³n de **SoluciÃ³n de Problemas**
2. Lee `README_QLoRA.md` para aspectos avanzados
3. Verifica que tienes el token de Hugging Face correcto

---

## ğŸš€ PrÃ³ximos Pasos

1. âœ… Instala la aplicaciÃ³n
2. âœ… Prueba el chat
3. âœ… Sube documentos para RAG
4. âœ… (Opcional) Entrena con QLora
5. âœ… Â¡Sube a GitHub!

**Â¡Disfruta tu chatbot local! ğŸ‰**
"# proyecto-gemma" 
